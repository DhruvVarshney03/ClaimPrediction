{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "541eefac-b772-4b8f-8c9e-33f11b7085b1",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 4\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mnumpy\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpd\u001b[39;00m\n\u001b[1;32m----> 4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mapplications\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m ResNet50\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpreprocessing\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mimage\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m img_to_array\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mapplications\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mresnet50\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m preprocess_input\n",
      "File \u001b[1;32m~\\OneDrive\\Desktop\\notebook\\first_env\\Lib\\site-packages\\tensorflow\\__init__.py:467\u001b[0m\n\u001b[0;32m    465\u001b[0m     importlib\u001b[38;5;241m.\u001b[39mimport_module(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtf_keras.src.optimizers\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    466\u001b[0m   \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 467\u001b[0m     \u001b[43mimportlib\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mimport_module\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mkeras.src.optimizers\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    468\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (\u001b[38;5;167;01mImportError\u001b[39;00m, \u001b[38;5;167;01mAttributeError\u001b[39;00m):\n\u001b[0;32m    469\u001b[0m   \u001b[38;5;28;01mpass\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\importlib\\__init__.py:126\u001b[0m, in \u001b[0;36mimport_module\u001b[1;34m(name, package)\u001b[0m\n\u001b[0;32m    124\u001b[0m             \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[0;32m    125\u001b[0m         level \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m--> 126\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_bootstrap\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_gcd_import\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m[\u001b[49m\u001b[43mlevel\u001b[49m\u001b[43m:\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpackage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlevel\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\OneDrive\\Desktop\\notebook\\first_env\\Lib\\site-packages\\keras\\__init__.py:2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# DO NOT EDIT. Generated by api_gen.sh\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mapi\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m DTypePolicy\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mapi\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m FloatDTypePolicy\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mapi\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Function\n",
      "File \u001b[1;32m~\\OneDrive\\Desktop\\notebook\\first_env\\Lib\\site-packages\\keras\\api\\__init__.py:8\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;124;03m\"\"\"DO NOT EDIT.\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \n\u001b[0;32m      3\u001b[0m \u001b[38;5;124;03mThis file was autogenerated. Do not edit it by hand,\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;124;03msince your modifications would be overwritten.\u001b[39;00m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m----> 8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mapi\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m activations\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mapi\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m applications\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mapi\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m backend\n",
      "File \u001b[1;32m~\\OneDrive\\Desktop\\notebook\\first_env\\Lib\\site-packages\\keras\\api\\activations\\__init__.py:7\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;124;03m\"\"\"DO NOT EDIT.\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \n\u001b[0;32m      3\u001b[0m \u001b[38;5;124;03mThis file was autogenerated. Do not edit it by hand,\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;124;03msince your modifications would be overwritten.\u001b[39;00m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m----> 7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msrc\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mactivations\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m deserialize\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msrc\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mactivations\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m get\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msrc\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mactivations\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m serialize\n",
      "File \u001b[1;32m~\\OneDrive\\Desktop\\notebook\\first_env\\Lib\\site-packages\\keras\\src\\__init__.py:13\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msrc\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m regularizers\n\u001b[0;32m     12\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msrc\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m utils\n\u001b[1;32m---> 13\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msrc\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m visualization\n\u001b[0;32m     14\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msrc\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbackend\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m KerasTensor\n\u001b[0;32m     15\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msrc\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlayers\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Input\n",
      "File \u001b[1;32m~\\OneDrive\\Desktop\\notebook\\first_env\\Lib\\site-packages\\keras\\src\\visualization\\__init__.py:2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msrc\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mvisualization\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m draw_bounding_boxes\n\u001b[1;32m----> 2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msrc\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mvisualization\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m plot_image_gallery\n",
      "File \u001b[1;32m~\\OneDrive\\Desktop\\notebook\\first_env\\Lib\\site-packages\\keras\\src\\visualization\\plot_image_gallery.py:13\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msrc\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlayers\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpreprocessing\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mimage_preprocessing\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbase_image_preprocessing_layer\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (  \u001b[38;5;66;03m# noqa: E501\u001b[39;00m\n\u001b[0;32m      9\u001b[0m     BaseImagePreprocessingLayer,\n\u001b[0;32m     10\u001b[0m )\n\u001b[0;32m     12\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 13\u001b[0m     \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpyplot\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mplt\u001b[39;00m\n\u001b[0;32m     14\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m:\n\u001b[0;32m     15\u001b[0m     plt \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32m~\\OneDrive\\Desktop\\notebook\\first_env\\Lib\\site-packages\\matplotlib\\pyplot.py:57\u001b[0m\n\u001b[0;32m     55\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mcycler\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m cycler  \u001b[38;5;66;03m# noqa: F401\u001b[39;00m\n\u001b[0;32m     56\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mmatplotlib\u001b[39;00m\n\u001b[1;32m---> 57\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcolorbar\u001b[39;00m\n\u001b[0;32m     58\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mimage\u001b[39;00m\n\u001b[0;32m     59\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m _api\n",
      "File \u001b[1;32m~\\OneDrive\\Desktop\\notebook\\first_env\\Lib\\site-packages\\matplotlib\\colorbar.py:19\u001b[0m\n\u001b[0;32m     16\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mnumpy\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[0;32m     18\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mmpl\u001b[39;00m\n\u001b[1;32m---> 19\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m _api, cbook, collections, cm, colors, contour, ticker\n\u001b[0;32m     20\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01martist\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mmartist\u001b[39;00m\n\u001b[0;32m     21\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpatches\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mmpatches\u001b[39;00m\n",
      "File \u001b[1;32m~\\OneDrive\\Desktop\\notebook\\first_env\\Lib\\site-packages\\matplotlib\\collections.py:21\u001b[0m\n\u001b[0;32m     18\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mnumpy\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[0;32m     20\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mmpl\u001b[39;00m\n\u001b[1;32m---> 21\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (_api, _path, artist, cbook, colorizer \u001b[38;5;28;01mas\u001b[39;00m mcolorizer, colors \u001b[38;5;28;01mas\u001b[39;00m mcolors,\n\u001b[0;32m     22\u001b[0m                _docstring, hatch \u001b[38;5;28;01mas\u001b[39;00m mhatch, lines \u001b[38;5;28;01mas\u001b[39;00m mlines, path \u001b[38;5;28;01mas\u001b[39;00m mpath, transforms)\n\u001b[0;32m     23\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_enums\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m JoinStyle, CapStyle\n\u001b[0;32m     26\u001b[0m \u001b[38;5;66;03m# \"color\" is excluded; it is a compound setter, and its docstring differs\u001b[39;00m\n\u001b[0;32m     27\u001b[0m \u001b[38;5;66;03m# in LineCollection.\u001b[39;00m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap>:1178\u001b[0m, in \u001b[0;36m_find_and_load\u001b[1;34m(name, import_)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap>:1149\u001b[0m, in \u001b[0;36m_find_and_load_unlocked\u001b[1;34m(name, import_)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap>:690\u001b[0m, in \u001b[0;36m_load_unlocked\u001b[1;34m(spec)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap_external>:936\u001b[0m, in \u001b[0;36mexec_module\u001b[1;34m(self, module)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap_external>:1032\u001b[0m, in \u001b[0;36mget_code\u001b[1;34m(self, fullname)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap_external>:1130\u001b[0m, in \u001b[0;36mget_data\u001b[1;34m(self, path)\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tensorflow.keras.applications import ResNet50\n",
    "from tensorflow.keras.preprocessing.image import img_to_array\n",
    "from tensorflow.keras.applications.resnet50 import preprocess_input\n",
    "import os\n",
    "from sklearn.preprocessing import StandardScaler, RobustScaler\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "import joblib\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "123c6fc6-20b1-4b8a-803d-3a848811cb8f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Load the cleaned structured data\n",
    "train_data_cleaned = pd.read_pickle('processed_data/cleaned_data/train_data_cleaned.pkl')\n",
    "test_data_cleaned = pd.read_pickle('processed_data/cleaned_data/test_data_cleaned.pkl')\n",
    "\n",
    "# Display the first few rows to confirm\n",
    "print(train_data_cleaned.head())\n",
    "print(test_data_cleaned.head())\n",
    "\n",
    "print(train_data_cleaned.shape)\n",
    "print(test_data_cleaned.shape)\n",
    "\n",
    "print(train_data_cleaned.dtypes)\n",
    "print(test_data_cleaned.dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "225f5859-ebef-4c26-8067-d23cdcc8baed",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train_data_cleaned['Expiry_date'].isnull().sum())  # To check if there are any missing paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "157400ff-e312-47d7-a21d-44311847a4d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(test_data_cleaned['Expiry_date'].isnull().sum())  # To check if there are any missing paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9f1097b-a993-4ff1-95b2-f69ff153080b",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train_data_cleaned['Insurance_company'].isnull().sum())  # Count missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bc05a86-8ae7-4be9-9ec5-1910996e9187",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f177a725-4f19-4882-935c-d253be12307e",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_cleaned.describe"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db4f63dd-c887-4eb6-a68c-b93df40b634a",
   "metadata": {},
   "source": [
    "### Turning Expiry_Date from an object to numeric value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12643708-86f3-45ee-93f2-842979d1b5b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(test_data_cleaned['Expiry_date'].dtype)  # Check column data type\n",
    "print(test_data_cleaned['Expiry_date'].head())  # Inspect the first few values\n",
    "print(train_data_cleaned['Expiry_date'].dtype)  # Check column data type\n",
    "print(train_data_cleaned['Expiry_date'].head())  # Inspect the first few values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "052ae644-8758-481f-941c-76783f31d7c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_expiry_date(df, date_column='Expiry_date'):\n",
    "    \"\"\"\n",
    "    Converts the 'Expiry_date' column from object to string, then standardizes the format to DD-MM-YYYY,\n",
    "    converts it to datetime, and finally converts it to numeric (epoch time).\n",
    "\n",
    "    Parameters:\n",
    "    df (DataFrame): The dataframe containing the date column.\n",
    "    date_column (str): The column name containing the expiry date.\n",
    "\n",
    "    Returns:\n",
    "    DataFrame: The dataframe with the converted expiry date in numeric format (epoch time).\n",
    "    \"\"\"\n",
    "\n",
    "    # Step 1: Ensure the column is treated as a string\n",
    "    df[date_column] = df[date_column].astype(str)\n",
    "\n",
    "    # Step 2: Detect the date format (YYYY-MM-DD or DD-MM-YYYY)\n",
    "    sample_date = df[date_column].dropna().iloc[0]  # Get a non-null sample date\n",
    "    \n",
    "    if '-' in sample_date:\n",
    "        date_parts = sample_date.split('-')\n",
    "        if len(date_parts[0]) == 4:  # YYYY-MM-DD format detected\n",
    "            print(\"Detected YYYY-MM-DD format. Converting to DD-MM-YYYY.\")\n",
    "            df[date_column] = pd.to_datetime(df[date_column], format='%Y-%m-%d', errors='coerce').dt.strftime('%d-%m-%Y')\n",
    "        else:\n",
    "            print(\"Detected DD-MM-YYYY format. No conversion needed.\")\n",
    "\n",
    "    # Step 3: Convert to datetime format\n",
    "    df[date_column] = pd.to_datetime(df[date_column], format='%d-%m-%Y', errors='coerce')\n",
    "\n",
    "    # Step 4: Convert datetime to numeric (epoch time)\n",
    "    df[date_column] = df[date_column].astype('int64') // 10**9  # Convert to Unix timestamp (seconds)\n",
    "\n",
    "    print(f\"Converted {date_column} to epoch time successfully.\")\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fc72bfb-ec90-4ba4-9e87-4c567a374c67",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_cleaned = convert_expiry_date(train_data_cleaned)\n",
    "test_data_cleaned = convert_expiry_date(test_data_cleaned)\n",
    "print(train_data_cleaned['Expiry_date'])\n",
    "print(test_data_cleaned['Expiry_date'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bddc405-a3b7-4595-ad30-92e53be97d1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check rows where Expiry_date is invalid\n",
    "invalid_dates = train_data_cleaned[train_data_cleaned['Expiry_date'].isna()]\n",
    "print(invalid_dates[['Expiry_date']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ddb65c5-f989-4bff-a366-89aa3a6b69ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train_data_cleaned.dtypes)\n",
    "print(test_data_cleaned.dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0d33c75-587f-4624-8744-8a4cd27dae20",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train_data_cleaned['Expiry_date'].isnull().sum())  # To check if there are any missing paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8858355c-1dbc-4ed2-9f3d-2c817edc9660",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(test_data_cleaned['Expiry_date'].isnull().sum())  # To check if there are any missing paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a655e8f4-7624-434a-ae0c-a0c9844130dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train_data_cleaned['Expiry_date'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0690594f-b5e5-4a43-b4fb-0074c1337192",
   "metadata": {},
   "source": [
    "### Clipping Amount"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4c57787-dff2-4754-bd4f-ab9b18e4c5e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Compute Q1 (25th percentile) and Q3 (75th percentile)\n",
    "Q1 = train_data_cleaned['Amount'].quantile(0.25)\n",
    "Q3 = train_data_cleaned['Amount'].quantile(0.75)\n",
    "\n",
    "# Compute IQR\n",
    "IQR = Q3 - Q1\n",
    "\n",
    "# Define lower and upper bounds\n",
    "lower_bound = max(0,Q1 - 1.5 * IQR)\n",
    "upper_bound = Q3 + 1.5 * IQR\n",
    "\n",
    "print(f\"Lower Bound: {lower_bound}, Upper Bound: {upper_bound}\")\n",
    "\n",
    "# Clip values\n",
    "train_data_cleaned['Amount'] = np.clip(train_data_cleaned['Amount'], lower_bound, upper_bound)\n",
    "\n",
    "# Plot new box plot\n",
    "plt.figure(figsize=(8, 4))\n",
    "sns.boxplot(x=train_data_cleaned['Amount'])\n",
    "plt.title(\"Boxplot for Amount (After Clipping)\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "307aaf9f-04d2-4d4d-bb73-129f48e012c1",
   "metadata": {},
   "source": [
    "### Feature Scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d861e2d-e6e2-4221-8adc-3f624c93085b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize scalers\n",
    "feature_scaler = StandardScaler()\n",
    "amount_scaler = StandardScaler()\n",
    "\n",
    "# Define feature columns (excluding 'Amount' initially)\n",
    "feature_columns = ['Cost_of_vehicle', 'Min_coverage', 'Max_coverage', 'Expiry_date']\n",
    "\n",
    "# Fit and transform feature columns\n",
    "train_data_cleaned[feature_columns] = feature_scaler.fit_transform(train_data_cleaned[feature_columns])\n",
    "\n",
    "# Apply log transformation BEFORE scaling\n",
    "train_data_cleaned['Amount'] = np.log1p(train_data_cleaned['Amount'])  # log(Amount + 1)\n",
    "\n",
    "# Scale 'Amount' separately\n",
    "train_data_cleaned['Amount'] = amount_scaler.fit_transform(train_data_cleaned[['Amount']].values.reshape(-1, 1))\n",
    "\n",
    "# Print shape and check values\n",
    "print(train_data_cleaned.shape)\n",
    "print(train_data_cleaned.tail())\n",
    "\n",
    "# Save scalers\n",
    "joblib.dump(feature_scaler, 'scalers/feature_scaler.pkl')\n",
    "joblib.dump(amount_scaler, 'scalers/amount_scaler.pkl')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a0edef1-3bda-4c40-80d7-e92403752f1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply the same transformation to test data (except 'Amount', which is not available)\n",
    "test_data_cleaned[feature_columns] = feature_scaler.transform(test_data_cleaned[feature_columns])\n",
    "print(test_data_cleaned.tail())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a6cf882-40ab-449a-b080-f92753304aed",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_cleaned['Amount'].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50743c28-7544-418d-bebc-65c5eafc6f32",
   "metadata": {},
   "source": [
    "### One-Hot Encoding Insurance company"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f14fffd-e52b-4421-b73d-78820f7912ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "ENCODER_PATH = \"scalers/encoder.pkl\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d8b896f-203f-4680-9fe6-090ee24194d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# **Load and apply One-Hot Encoding**\n",
    "if not os.path.exists(ENCODER_PATH):\n",
    "    raise FileNotFoundError(f\"Encoder file not found: {ENCODER_PATH}\")\n",
    "    \n",
    "with open(ENCODER_PATH, \"rb\") as f:\n",
    "        encoder = joblib.load(f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b49cd85b-7ce9-4cc0-a103-595c520bc58f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit the encoder on the training data and transform the 'Insurance_company' column in the train dataset\n",
    "train_data_encoded = encoder.transform(train_data_cleaned[['Insurance_company']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14dfdb5e-9063-4b5e-a5cb-3801fa87f5c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the encoded data back to a DataFrame\n",
    "train_data_encoded = pd.DataFrame(train_data_encoded, columns=encoder.get_feature_names_out(['Insurance_company']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4efdee63-dce8-46fb-a86d-575302d28dc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train_data_encoded.shape)\n",
    "print(train_data_encoded.tail())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3659182-04dd-4a53-8745-f620284e3c8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop the original 'Insurance_company' column from train_data_cleaned\n",
    "train_data_cleaned = train_data_cleaned.drop(columns=['Insurance_company'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4280c896-effd-4f41-b7d3-c8055c1423e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Join the one-hot encoded columns with the original DataFrame\n",
    "train_data_cleaned = train_data_cleaned.join(train_data_encoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9ba9e87-7d5f-4785-91c0-02f5f2ddea31",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train_data_cleaned.shape)\n",
    "print(train_data_cleaned.tail())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8b1a989-f5b1-40eb-86e6-98945774b378",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform the 'Insurance_company' column in the test dataset using the same encoder\n",
    "test_data_encoded = encoder.transform(test_data_cleaned[['Insurance_company']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab6ab188-81a6-42fa-bec6-eedba2e827a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the transformed data back to a DataFrame\n",
    "test_data_encoded = pd.DataFrame(test_data_encoded, columns=encoder.get_feature_names_out(['Insurance_company']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34907fc4-a70b-451e-ac10-855ed338431b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop the original 'Insurance_company' column from test_data_cleaned\n",
    "test_data_cleaned = test_data_cleaned.drop(columns=['Insurance_company'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05805338-7011-4c45-bb61-382b5a30f047",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Join the one-hot encoded columns with the test dataset\n",
    "test_data_cleaned = test_data_cleaned.join(test_data_encoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31e378db-27c7-449e-9c6e-7711ccb8897a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train_data_cleaned.shape)\n",
    "print(test_data_cleaned.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93531d40-0456-409f-9f6d-24313f2c15a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train_data_cleaned.tail())\n",
    "print(test_data_cleaned.tail())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6193f53f-ad5a-4cdd-926d-2ef70ce3220f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train_data_cleaned['Insurance_company_A'].isnull().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1397b33-fcd2-4443-9436-ae9d1715c9a3",
   "metadata": {},
   "source": [
    "### Image Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e63de076-08a9-490c-b8e3-dd56704f1418",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "\n",
    "# # Folder containing all images\n",
    "# image_folder = \"Fast_Furious_Insured/images/train_images\"\n",
    "\n",
    "# # Get the list of images that are still relevant\n",
    "# valid_images = set(train_data_cleaned[\"Image_path\"].apply(os.path.basename))  # Extract filenames only\n",
    "\n",
    "# # List all files in the folder\n",
    "# all_images = set(os.listdir(image_folder))\n",
    "\n",
    "# # Find extra images that need to be deleted\n",
    "# extra_images = all_images - valid_images\n",
    "\n",
    "# # Delete extra images\n",
    "# for img in extra_images:\n",
    "#     img_path = os.path.join(image_folder, img)\n",
    "#     os.remove(img_path)  # Deletes the file\n",
    "#     print(f\"Deleted: {img_path}\")\n",
    "\n",
    "# print(f\"✅ Removed {len(extra_images)} unnecessary images.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7038ddcd-e352-4be7-b21d-10a5d937e4a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#No. of Images\n",
    "train_images_path = \"images/train_images\"\n",
    "test_images_path = \"images/test_images\"\n",
    "\n",
    "num_train_images = len(os.listdir(train_images_path))\n",
    "num_test_images = len(os.listdir(test_images_path))\n",
    "\n",
    "print(f\"Number of training images: {num_train_images}\")\n",
    "print(f\"Number of test images: {num_test_images}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "052269cf-3830-4f84-bb77-268c31064730",
   "metadata": {},
   "outputs": [],
   "source": [
    "IMG_SIZE = (224, 224)  # ResNet50 requires images to be 224x224\n",
    "\n",
    "def preprocess_image(image_path):\n",
    "    # Load the image using OpenCV\n",
    "    image = cv2.imread(image_path)\n",
    "    \n",
    "    # Resize the image to the target size (224x224 for ResNet50)\n",
    "    image_resized = cv2.resize(image, IMG_SIZE)\n",
    "    \n",
    "    # Convert image to float32 for normalization\n",
    "    image_normalized = image_resized.astype('float32') / 255.0\n",
    "    \n",
    "    # Preprocess the image using ResNet50 preprocessing (mean subtraction, etc.)\n",
    "    image_processed = preprocess_input(image_normalized)\n",
    "    \n",
    "    return image_processed\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6642ae32-b0a2-4e81-817d-4a4fd0f4e57c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the pre-trained ResNet50 model (exclude top layers for feature extraction)\n",
    "base_model = ResNet50(weights='imagenet', include_top=False, pooling='avg')\n",
    "\n",
    "def extract_image_features(image_path):\n",
    "    # Preprocess the image\n",
    "    image_processed = preprocess_image(image_path)\n",
    "    \n",
    "    # Expand dimensions for batch processing (ResNet50 expects a batch)\n",
    "    image_batch = np.expand_dims(image_processed, axis=0)\n",
    "    \n",
    "    # Extract features using the ResNet50 model\n",
    "    features = base_model.predict(image_batch)\n",
    "    \n",
    "    # Flatten the feature vector\n",
    "    return features.flatten()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe38d7d3-e36e-449f-8642-f170d0f5199a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def combine_features(image_paths, structured_data):\n",
    "    image_features = []\n",
    "    \n",
    "    # Iterate over all image paths and extract features\n",
    "    for path in image_paths:\n",
    "        features = extract_image_features(path)\n",
    "        image_features.append(features)\n",
    "    \n",
    "    # Convert image features into a DataFrame\n",
    "    image_features_df = pd.DataFrame(image_features, index=structured_data.index)\n",
    "\n",
    "     # Print to check before merging\n",
    "    print(\"Image Features Shape:\", image_features_df.shape)\n",
    "    print(\"Structured Data Shape:\", structured_data.shape)\n",
    "    \n",
    "    # Concatenate image features with structured data (merge on index)\n",
    "    combined_data = structured_data.join(image_features_df)\n",
    "    \n",
    "    return combined_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f60f2605-541b-43ff-9c99-e0892239009c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the image paths from the 'Image_Path' column\n",
    "train_image_paths = train_data_cleaned['Image_path'].tolist()\n",
    "test_image_paths = test_data_cleaned['Image_path'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87fa90b4-d362-446c-abd9-8b962db6dd3c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Combine image features with the structured data\n",
    "train_combined = combine_features(train_image_paths, train_data_cleaned)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f04eae5-02dd-4bcb-9e56-ccc5191ed502",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#test_combined = combine_features(test_image_paths, test_data_cleaned)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14ddeacd-73ad-4774-8281-5b81a8acd7b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_combined.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f97586a4-421c-4be5-9e4a-1054b0405d53",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_combined.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8de7fbc1-1a50-4908-a29b-aa3a344f56a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Extract image filenames from DataFrame\n",
    "# train_images_used = set(train_data_cleaned[\"Image_path\"].apply(lambda x: x.split(\"/\")[-1]))\n",
    "\n",
    "# # List actual image files in the folder\n",
    "# import os\n",
    "# image_folder = \"Fast_Furious_Insured/images/train_images\"\n",
    "# all_image_files = set(os.listdir(image_folder))\n",
    "\n",
    "# # Find extra images that are not in train_data_cleaned\n",
    "# extra_images = all_image_files - train_images_used\n",
    "\n",
    "# print(f\"Extra images processed: {len(extra_images)}\")\n",
    "# print(f\"Sample extra images: {list(extra_images)[:5]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff54491d-44cd-4982-87e6-93480e793933",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "\n",
    "# # Get the list of image filenames in the folder\n",
    "# image_folder = \"Fast_Furious_Insured/images/train_images\"\n",
    "# all_image_files = set(os.listdir(image_folder))\n",
    "\n",
    "# # Extract filenames from the train dataset\n",
    "# train_image_filenames = set(train_data_cleaned[\"Image_path\"].apply(lambda x: os.path.basename(x)))\n",
    "\n",
    "# # Identify extra images (those in the folder but not in the dataset)\n",
    "# extra_images = all_image_files - train_image_filenames\n",
    "# missing_images = train_image_filenames - all_image_files\n",
    "\n",
    "# print(f\"Extra images in the folder: {len(extra_images)}\")\n",
    "# print(f\"Missing images: {len(missing_images)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fb7dbc2-0271-49be-84bc-c5ddae2f96b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Filter out rows where the image file doesn't exist\n",
    "# valid_image_paths = set(train_data_cleaned[\"Image_path\"].apply(lambda x: os.path.basename(x)))\n",
    "# all_images_in_folder = set(os.listdir(image_folder))\n",
    "\n",
    "# # Keep only records with matching image files\n",
    "# train_data_cleaned_filtered = train_data_cleaned[train_data_cleaned[\"Image_path\"].apply(\n",
    "#     lambda x: os.path.basename(x) in all_images_in_folder\n",
    "# )]\n",
    "\n",
    "# print(f\"Filtered train data shape: {train_data_cleaned_filtered.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4a7511b-5a47-4053-9ce8-a99610122724",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the number of unique image paths in the cleaned dataset and processed data\n",
    "print(f\"Unique image paths in train_data_cleaned: {train_data_cleaned['Image_path'].nunique()}\")\n",
    "print(f\"Unique image paths in processed train_combined: {train_combined['Image_path'].nunique()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc322c53-a995-4187-ad15-4747866492a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(test_combined['Image_path'].isnull().sum())  # To check if there are any missing paths\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00072dbc-029d-4d29-982d-5296777dd150",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for duplicates based on all columns in the combined dataset\n",
    "duplicates = train_combined[train_combined.duplicated(subset='Image_path', keep=False)]  # Keep=False to mark all duplicates\n",
    "print(duplicates)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cee80fd0-50e9-434b-81d9-f072ba9191fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the indexes of duplicated rows\n",
    "duplicate_indexes = train_combined[train_combined.duplicated(subset='Image_path',keep=False)].index\n",
    "print(\"Duplicate indexes:\", duplicate_indexes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17d94ead-a938-4958-8fb1-c10c521ef373",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Length of duplicate_indexes: {len(duplicate_indexes)}\")\n",
    "print(f\"Length of train_image_paths: {len(train_image_paths)}\")\n",
    "print(f\"Shape of train_combined: {train_combined.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "618c6689-4a47-476b-83db-cbfc2178c7c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_combined.to_csv('processed_data/train_combined.csv', index=False)\n",
    "test_combined.to_csv('processed_data/test_combined.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c8f469e-9338-4865-be31-785cb7450b5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_combined.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6c5b78b-b027-4f62-8698-d27b9bbf9ae6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save train_combined as pickle file\n",
    "train_combined.to_pickle('processed_data/final_train_data.pkl')\n",
    "\n",
    "# Save test_combined as pickle file\n",
    "test_combined.to_pickle('processed_data/final_test_data.pkl')\n",
    "\n",
    "print(\"Pickle files saved successfully!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.11",
   "language": "python",
   "name": "python3.11"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
