{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "541eefac-b772-4b8f-8c9e-33f11b7085b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tensorflow.keras.applications import ResNet50\n",
    "from tensorflow.keras.preprocessing.image import img_to_array\n",
    "from tensorflow.keras.applications.resnet50 import preprocess_input\n",
    "import os\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "import joblib\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "123c6fc6-20b1-4b8a-803d-3a848811cb8f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                          Image_path Insurance_company  \\\n",
      "0  Fast_Furious_Insured/images/train_images/img_4...                BQ   \n",
      "1  Fast_Furious_Insured/images/train_images/img_7...                BQ   \n",
      "2  Fast_Furious_Insured/images/train_images/img_4...                 A   \n",
      "3  Fast_Furious_Insured/images/train_images/img_7...                 A   \n",
      "4  Fast_Furious_Insured/images/train_images/img_7...                AC   \n",
      "\n",
      "   Cost_of_vehicle  Min_coverage Expiry_date  Max_coverage  Condition  Amount  \n",
      "0          41500.0        1037.5  03-12-2026      36142.68          0     0.0  \n",
      "1          50700.0        1267.5  10-07-2025      12753.00          1  6194.0  \n",
      "2          49500.0        1237.5  11-08-2022      43102.68          0     0.0  \n",
      "3          33500.0         837.5  02-08-2022       8453.00          1  7699.0  \n",
      "4          27600.0         690.0  01-05-2026       6978.00          1  8849.0  \n",
      "                                          Image_path Insurance_company  \\\n",
      "0  Fast_Furious_Insured/images/test_images/img_45...                 B   \n",
      "1  Fast_Furious_Insured/images/test_images/img_77...                 C   \n",
      "2  Fast_Furious_Insured/images/test_images/img_46...                AC   \n",
      "3  Fast_Furious_Insured/images/test_images/img_45...                BB   \n",
      "4  Fast_Furious_Insured/images/test_images/img_45...                BB   \n",
      "\n",
      "   Cost_of_vehicle  Min_coverage Expiry_date  Max_coverage  \n",
      "0            23600         590.0  12-04-2025        5978.0  \n",
      "1            28300         707.5  24-08-2028        7153.0  \n",
      "2            43700        1092.5  28-11-2023       11003.0  \n",
      "3            46100        1152.5  04-02-2028       11603.0  \n",
      "4            40700        1017.5  03-01-2022       10253.0  \n",
      "(1310, 8)\n",
      "(600, 6)\n",
      "Image_path            object\n",
      "Insurance_company     object\n",
      "Cost_of_vehicle      float64\n",
      "Min_coverage         float64\n",
      "Expiry_date           object\n",
      "Max_coverage         float64\n",
      "Condition              int64\n",
      "Amount               float64\n",
      "dtype: object\n",
      "Image_path            object\n",
      "Insurance_company     object\n",
      "Cost_of_vehicle        int64\n",
      "Min_coverage         float64\n",
      "Expiry_date           object\n",
      "Max_coverage         float64\n",
      "dtype: object\n"
     ]
    }
   ],
   "source": [
    "# Load the cleaned structured data\n",
    "train_data_cleaned = pd.read_pickle('Fast_Furious_Insured/processed_data/train_data_cleaned.pkl')\n",
    "test_data_cleaned = pd.read_pickle('Fast_Furious_Insured/processed_data/test_data_cleaned.pkl')\n",
    "\n",
    "# Display the first few rows to confirm\n",
    "print(train_data_cleaned.head())\n",
    "print(test_data_cleaned.head())\n",
    "\n",
    "print(train_data_cleaned.shape)\n",
    "print(test_data_cleaned.shape)\n",
    "\n",
    "print(train_data_cleaned.dtypes)\n",
    "print(test_data_cleaned.dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "225f5859-ebef-4c26-8067-d23cdcc8baed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "print(train_data_cleaned['Expiry_date'].isnull().sum())  # To check if there are any missing paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "157400ff-e312-47d7-a21d-44311847a4d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "print(test_data_cleaned['Expiry_date'].isnull().sum())  # To check if there are any missing paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e9f1097b-a993-4ff1-95b2-f69ff153080b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "print(train_data_cleaned['Insurance_company'].isnull().sum())  # Count missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1bc05a86-8ae7-4be9-9ec5-1910996e9187",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "object\n"
     ]
    }
   ],
   "source": [
    "print(train_data_cleaned['Insurance_company'].dtype)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f177a725-4f19-4882-935c-d253be12307e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skewness: 4.352416666529772\n"
     ]
    }
   ],
   "source": [
    "from scipy.stats import skew\n",
    "\n",
    "skewness = skew(train_data_cleaned['Amount'])\n",
    "print(f\"Skewness: {skewness}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db4f63dd-c887-4eb6-a68c-b93df40b634a",
   "metadata": {},
   "source": [
    "### Turning Expiry_Date from an object to numeric value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "12643708-86f3-45ee-93f2-842979d1b5b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "object\n",
      "0    12-04-2025\n",
      "1    24-08-2028\n",
      "2    28-11-2023\n",
      "3    04-02-2028\n",
      "4    03-01-2022\n",
      "Name: Expiry_date, dtype: object\n",
      "object\n",
      "0    03-12-2026\n",
      "1    10-07-2025\n",
      "2    11-08-2022\n",
      "3    02-08-2022\n",
      "4    01-05-2026\n",
      "Name: Expiry_date, dtype: object\n"
     ]
    }
   ],
   "source": [
    "print(test_data_cleaned['Expiry_date'].dtype)  # Check column data type\n",
    "print(test_data_cleaned['Expiry_date'].head())  # Inspect the first few values\n",
    "print(train_data_cleaned['Expiry_date'].dtype)  # Check column data type\n",
    "print(train_data_cleaned['Expiry_date'].head())  # Inspect the first few values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "052ae644-8758-481f-941c-76783f31d7c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_expiry_date(df, date_column='Expiry_date'):\n",
    "    \"\"\"\n",
    "    Converts the 'Expiry_date' column from object to string, then standardizes the format to DD-MM-YYYY,\n",
    "    converts it to datetime, and finally converts it to numeric (epoch time).\n",
    "\n",
    "    Parameters:\n",
    "    df (DataFrame): The dataframe containing the date column.\n",
    "    date_column (str): The column name containing the expiry date.\n",
    "\n",
    "    Returns:\n",
    "    DataFrame: The dataframe with the converted expiry date in numeric format (epoch time).\n",
    "    \"\"\"\n",
    "\n",
    "    # Step 1: Ensure the column is treated as a string\n",
    "    df[date_column] = df[date_column].astype(str)\n",
    "\n",
    "    # Step 2: Detect the date format (YYYY-MM-DD or DD-MM-YYYY)\n",
    "    sample_date = df[date_column].dropna().iloc[0]  # Get a non-null sample date\n",
    "    \n",
    "    if '-' in sample_date:\n",
    "        date_parts = sample_date.split('-')\n",
    "        if len(date_parts[0]) == 4:  # YYYY-MM-DD format detected\n",
    "            print(\"Detected YYYY-MM-DD format. Converting to DD-MM-YYYY.\")\n",
    "            df[date_column] = pd.to_datetime(df[date_column], format='%Y-%m-%d', errors='coerce').dt.strftime('%d-%m-%Y')\n",
    "        else:\n",
    "            print(\"Detected DD-MM-YYYY format. No conversion needed.\")\n",
    "\n",
    "    # Step 3: Convert to datetime format\n",
    "    df[date_column] = pd.to_datetime(df[date_column], format='%d-%m-%Y', errors='coerce')\n",
    "\n",
    "    # Step 4: Convert datetime to numeric (epoch time)\n",
    "    df[date_column] = df[date_column].astype('int64') // 10**9  # Convert to Unix timestamp (seconds)\n",
    "\n",
    "    print(f\"Converted {date_column} to epoch time successfully.\")\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7fc72bfb-ec90-4ba4-9e87-4c567a374c67",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Detected DD-MM-YYYY format. No conversion needed.\n",
      "Converted Expiry_date to epoch time successfully.\n",
      "Detected DD-MM-YYYY format. No conversion needed.\n",
      "Converted Expiry_date to epoch time successfully.\n",
      "0       1796256000\n",
      "1       1752105600\n",
      "2       1660176000\n",
      "3       1659398400\n",
      "4       1777593600\n",
      "           ...    \n",
      "1305    1641513600\n",
      "1306    1739750400\n",
      "1307    1703894400\n",
      "1308    1669334400\n",
      "1309    1665360000\n",
      "Name: Expiry_date, Length: 1310, dtype: int64\n",
      "0      1744416000\n",
      "1      1850688000\n",
      "2      1701129600\n",
      "3      1833235200\n",
      "4      1641168000\n",
      "          ...    \n",
      "595    1729641600\n",
      "596    1740096000\n",
      "597    1689206400\n",
      "598    1714867200\n",
      "599    1754524800\n",
      "Name: Expiry_date, Length: 600, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "train_data_cleaned = convert_expiry_date(train_data_cleaned)\n",
    "test_data_cleaned = convert_expiry_date(test_data_cleaned)\n",
    "print(train_data_cleaned['Expiry_date'])\n",
    "print(test_data_cleaned['Expiry_date'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3bddc405-a3b7-4595-ad30-92e53be97d1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Empty DataFrame\n",
      "Columns: [Expiry_date]\n",
      "Index: []\n"
     ]
    }
   ],
   "source": [
    "# Check rows where Expiry_date is invalid\n",
    "invalid_dates = train_data_cleaned[train_data_cleaned['Expiry_date'].isna()]\n",
    "print(invalid_dates[['Expiry_date']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7ddb65c5-f989-4bff-a366-89aa3a6b69ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image_path            object\n",
      "Insurance_company     object\n",
      "Cost_of_vehicle      float64\n",
      "Min_coverage         float64\n",
      "Expiry_date            int64\n",
      "Max_coverage         float64\n",
      "Condition              int64\n",
      "Amount               float64\n",
      "dtype: object\n",
      "Image_path            object\n",
      "Insurance_company     object\n",
      "Cost_of_vehicle        int64\n",
      "Min_coverage         float64\n",
      "Expiry_date            int64\n",
      "Max_coverage         float64\n",
      "dtype: object\n"
     ]
    }
   ],
   "source": [
    "print(train_data_cleaned.dtypes)\n",
    "print(test_data_cleaned.dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a0d33c75-587f-4624-8744-8a4cd27dae20",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "print(train_data_cleaned['Expiry_date'].isnull().sum())  # To check if there are any missing paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8858355c-1dbc-4ed2-9f3d-2c817edc9660",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "print(test_data_cleaned['Expiry_date'].isnull().sum())  # To check if there are any missing paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a655e8f4-7624-434a-ae0c-a0c9844130dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0       1796256000\n",
      "1       1752105600\n",
      "2       1660176000\n",
      "3       1659398400\n",
      "4       1777593600\n",
      "           ...    \n",
      "1305    1641513600\n",
      "1306    1739750400\n",
      "1307    1703894400\n",
      "1308    1669334400\n",
      "1309    1665360000\n",
      "Name: Expiry_date, Length: 1310, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(train_data_cleaned['Expiry_date'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "307aaf9f-04d2-4d4d-bb73-129f48e012c1",
   "metadata": {},
   "source": [
    "### Feature Scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5d861e2d-e6e2-4221-8adc-3f624c93085b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1310, 8)\n",
      "                                             Image_path Insurance_company  \\\n",
      "1305  Fast_Furious_Insured/images/train_images/img_4...                AC   \n",
      "1306  Fast_Furious_Insured/images/train_images/img_4...                DA   \n",
      "1307  Fast_Furious_Insured/images/train_images/img_4...                BQ   \n",
      "1308  Fast_Furious_Insured/images/train_images/img_4...                AA   \n",
      "1309  Fast_Furious_Insured/images/train_images/img_4...                 A   \n",
      "\n",
      "      Cost_of_vehicle  Min_coverage  Expiry_date  Max_coverage  Condition  \\\n",
      "1305         1.574982      1.574982    -1.611635      0.238804          1   \n",
      "1306         1.664688      1.664688    -0.147302      0.266733          1   \n",
      "1307         0.453657      0.453657    -0.681777     -0.110309          1   \n",
      "1308        -0.678881     -0.678881    -1.196934     -0.462914          1   \n",
      "1309        -0.477042     -0.477042    -1.256177     -0.400073          1   \n",
      "\n",
      "        Amount  \n",
      "1305  0.356944  \n",
      "1306  0.408026  \n",
      "1307  0.275436  \n",
      "1308  0.475741  \n",
      "1309  0.689561  \n"
     ]
    }
   ],
   "source": [
    "# Define feature columns (excluding 'Amount' initially)\n",
    "feature_columns = ['Cost_of_vehicle', 'Min_coverage', 'Max_coverage', 'Expiry_date']\n",
    "\n",
    "# Initialize the scaler for features\n",
    "feature_scaler = StandardScaler()\n",
    "\n",
    "# Fit and transform feature columns\n",
    "train_data_cleaned[feature_columns] = feature_scaler.fit_transform(train_data_cleaned[feature_columns])\n",
    "\n",
    "# Apply log transformation before scaling\n",
    "train_data_cleaned['Amount'] = np.log1p(train_data_cleaned['Amount'])  # log(Amount + 1)\n",
    "\n",
    "# Scale 'Amount' separately using another scaler\n",
    "amount_scaler = StandardScaler()\n",
    "train_data_cleaned['Amount'] = amount_scaler.fit_transform(train_data_cleaned[['Amount']])\n",
    "\n",
    "# Save the scalers for later use\n",
    "joblib.dump(feature_scaler, 'feature_scaler.pkl')\n",
    "joblib.dump(amount_scaler, 'amount_scaler.pkl')\n",
    "print(train_data_cleaned.shape)\n",
    "print(train_data_cleaned.tail())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0a0edef1-3bda-4c40-80d7-e92403752f1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                            Image_path Insurance_company  \\\n",
      "595  Fast_Furious_Insured/images/test_images/img_77...                 B   \n",
      "596  Fast_Furious_Insured/images/test_images/img_45...                 O   \n",
      "597  Fast_Furious_Insured/images/test_images/img_45...                BQ   \n",
      "598  Fast_Furious_Insured/images/test_images/img_45...                AA   \n",
      "599  Fast_Furious_Insured/images/test_images/img_46...                RE   \n",
      "\n",
      "     Cost_of_vehicle  Min_coverage  Expiry_date  Max_coverage  \n",
      "595        -0.734947     -0.734947    -0.297986     -0.480369  \n",
      "596         1.552556      1.552556    -0.142151      0.231822  \n",
      "597        -1.172264     -1.172264    -0.900719      1.715508  \n",
      "598         0.577003      0.577003    -0.518215     -0.071907  \n",
      "599         1.328291      1.328291     0.072927      0.161999  \n"
     ]
    }
   ],
   "source": [
    "# Apply the same transformation to test data (except 'Amount', which is not available)\n",
    "test_data_cleaned[feature_columns] = feature_scaler.transform(test_data_cleaned[feature_columns])\n",
    "print(test_data_cleaned.tail())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50743c28-7544-418d-bebc-65c5eafc6f32",
   "metadata": {},
   "source": [
    "### One-Hot Encoding Insurance company"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b49cd85b-7ce9-4cc0-a103-595c520bc58f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the encoder\n",
    "encoder = OneHotEncoder(sparse_output=False, handle_unknown='error')\n",
    "\n",
    "# Fit the encoder on the training data and transform the 'Insurance_company' column in the train dataset\n",
    "train_data_encoded = encoder.fit_transform(train_data_cleaned[['Insurance_company']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "14dfdb5e-9063-4b5e-a5cb-3801fa87f5c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the encoded data back to a DataFrame\n",
    "train_data_encoded = pd.DataFrame(train_data_encoded, columns=encoder.get_feature_names_out(['Insurance_company']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d0fd324a-301c-4d21-915e-f6c1c5bfeb61",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open(r\"C:\\Users\\varsh\\OneDrive\\Desktop\\notebook\\Fast_Furious_Insured\\encoder.pkl\", \"wb\") as f:\n",
    "    pickle.dump(encoder, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4efdee63-dce8-46fb-a86d-575302d28dc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train_data_encoded.shape)\n",
    "print(train_data_encoded.tail())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3659182-04dd-4a53-8745-f620284e3c8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop the original 'Insurance_company' column from train_data_cleaned\n",
    "train_data_cleaned = train_data_cleaned.drop(columns=['Insurance_company'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4280c896-effd-4f41-b7d3-c8055c1423e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Join the one-hot encoded columns with the original DataFrame\n",
    "train_data_cleaned = train_data_cleaned.join(train_data_encoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9ba9e87-7d5f-4785-91c0-02f5f2ddea31",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train_data_cleaned.shape)\n",
    "print(train_data_cleaned.tail())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8b1a989-f5b1-40eb-86e6-98945774b378",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform the 'Insurance_company' column in the test dataset using the same encoder\n",
    "test_data_encoded = encoder.transform(test_data_cleaned[['Insurance_company']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab6ab188-81a6-42fa-bec6-eedba2e827a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the transformed data back to a DataFrame\n",
    "test_data_encoded = pd.DataFrame(test_data_encoded, columns=encoder.get_feature_names_out(['Insurance_company']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34907fc4-a70b-451e-ac10-855ed338431b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop the original 'Insurance_company' column from test_data_cleaned\n",
    "test_data_cleaned = test_data_cleaned.drop(columns=['Insurance_company'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05805338-7011-4c45-bb61-382b5a30f047",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Join the one-hot encoded columns with the test dataset\n",
    "test_data_cleaned = test_data_cleaned.join(test_data_encoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31e378db-27c7-449e-9c6e-7711ccb8897a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train_data_cleaned.shape)\n",
    "print(test_data_cleaned.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93531d40-0456-409f-9f6d-24313f2c15a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train_data_cleaned.tail())\n",
    "print(test_data_cleaned.tail())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6193f53f-ad5a-4cdd-926d-2ef70ce3220f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train_data_cleaned['Insurance_company_A'].isnull().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1397b33-fcd2-4443-9436-ae9d1715c9a3",
   "metadata": {},
   "source": [
    "### Image Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e63de076-08a9-490c-b8e3-dd56704f1418",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "\n",
    "# # Folder containing all images\n",
    "# image_folder = \"Fast_Furious_Insured/images/train_images\"\n",
    "\n",
    "# # Get the list of images that are still relevant\n",
    "# valid_images = set(train_data_cleaned[\"Image_path\"].apply(os.path.basename))  # Extract filenames only\n",
    "\n",
    "# # List all files in the folder\n",
    "# all_images = set(os.listdir(image_folder))\n",
    "\n",
    "# # Find extra images that need to be deleted\n",
    "# extra_images = all_images - valid_images\n",
    "\n",
    "# # Delete extra images\n",
    "# for img in extra_images:\n",
    "#     img_path = os.path.join(image_folder, img)\n",
    "#     os.remove(img_path)  # Deletes the file\n",
    "#     print(f\"Deleted: {img_path}\")\n",
    "\n",
    "# print(f\"✅ Removed {len(extra_images)} unnecessary images.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7038ddcd-e352-4be7-b21d-10a5d937e4a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#No. of Images\n",
    "train_images_path = \"Fast_Furious_Insured/images/train_images\"\n",
    "test_images_path = \"Fast_Furious_Insured/images/test_images\"\n",
    "\n",
    "num_train_images = len(os.listdir(train_images_path))\n",
    "num_test_images = len(os.listdir(test_images_path))\n",
    "\n",
    "print(f\"Number of training images: {num_train_images}\")\n",
    "print(f\"Number of test images: {num_test_images}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "052269cf-3830-4f84-bb77-268c31064730",
   "metadata": {},
   "outputs": [],
   "source": [
    "IMG_SIZE = (224, 224)  # ResNet50 requires images to be 224x224\n",
    "\n",
    "def preprocess_image(image_path):\n",
    "    # Load the image using OpenCV\n",
    "    image = cv2.imread(image_path)\n",
    "    \n",
    "    # Resize the image to the target size (224x224 for ResNet50)\n",
    "    image_resized = cv2.resize(image, IMG_SIZE)\n",
    "    \n",
    "    # Convert image to float32 for normalization\n",
    "    image_normalized = image_resized.astype('float32') / 255.0\n",
    "    \n",
    "    # Preprocess the image using ResNet50 preprocessing (mean subtraction, etc.)\n",
    "    image_processed = preprocess_input(image_normalized)\n",
    "    \n",
    "    return image_processed\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6642ae32-b0a2-4e81-817d-4a4fd0f4e57c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the pre-trained ResNet50 model (exclude top layers for feature extraction)\n",
    "base_model = ResNet50(weights='imagenet', include_top=False, pooling='avg')\n",
    "\n",
    "def extract_image_features(image_path):\n",
    "    # Preprocess the image\n",
    "    image_processed = preprocess_image(image_path)\n",
    "    \n",
    "    # Expand dimensions for batch processing (ResNet50 expects a batch)\n",
    "    image_batch = np.expand_dims(image_processed, axis=0)\n",
    "    \n",
    "    # Extract features using the ResNet50 model\n",
    "    features = base_model.predict(image_batch)\n",
    "    \n",
    "    # Flatten the feature vector\n",
    "    return features.flatten()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe38d7d3-e36e-449f-8642-f170d0f5199a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def combine_features(image_paths, structured_data):\n",
    "    image_features = []\n",
    "    \n",
    "    # Iterate over all image paths and extract features\n",
    "    for path in image_paths:\n",
    "        features = extract_image_features(path)\n",
    "        image_features.append(features)\n",
    "    \n",
    "    # Convert image features into a DataFrame\n",
    "    image_features_df = pd.DataFrame(image_features, index=structured_data.index)\n",
    "\n",
    "     # Print to check before merging\n",
    "    print(\"Image Features Shape:\", image_features_df.shape)\n",
    "    print(\"Structured Data Shape:\", structured_data.shape)\n",
    "    \n",
    "    # Concatenate image features with structured data (merge on index)\n",
    "    combined_data = structured_data.join(image_features_df)\n",
    "    \n",
    "    return combined_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f60f2605-541b-43ff-9c99-e0892239009c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the image paths from the 'Image_Path' column\n",
    "train_image_paths = train_data_cleaned['Image_path'].tolist()\n",
    "test_image_paths = test_data_cleaned['Image_path'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87fa90b4-d362-446c-abd9-8b962db6dd3c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Combine image features with the structured data\n",
    "train_combined = combine_features(train_image_paths, train_data_cleaned)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f04eae5-02dd-4bcb-9e56-ccc5191ed502",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "test_combined = combine_features(test_image_paths, test_data_cleaned)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14ddeacd-73ad-4774-8281-5b81a8acd7b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_combined.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f97586a4-421c-4be5-9e4a-1054b0405d53",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_combined.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8de7fbc1-1a50-4908-a29b-aa3a344f56a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Extract image filenames from DataFrame\n",
    "# train_images_used = set(train_data_cleaned[\"Image_path\"].apply(lambda x: x.split(\"/\")[-1]))\n",
    "\n",
    "# # List actual image files in the folder\n",
    "# import os\n",
    "# image_folder = \"Fast_Furious_Insured/images/train_images\"\n",
    "# all_image_files = set(os.listdir(image_folder))\n",
    "\n",
    "# # Find extra images that are not in train_data_cleaned\n",
    "# extra_images = all_image_files - train_images_used\n",
    "\n",
    "# print(f\"Extra images processed: {len(extra_images)}\")\n",
    "# print(f\"Sample extra images: {list(extra_images)[:5]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff54491d-44cd-4982-87e6-93480e793933",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "\n",
    "# # Get the list of image filenames in the folder\n",
    "# image_folder = \"Fast_Furious_Insured/images/train_images\"\n",
    "# all_image_files = set(os.listdir(image_folder))\n",
    "\n",
    "# # Extract filenames from the train dataset\n",
    "# train_image_filenames = set(train_data_cleaned[\"Image_path\"].apply(lambda x: os.path.basename(x)))\n",
    "\n",
    "# # Identify extra images (those in the folder but not in the dataset)\n",
    "# extra_images = all_image_files - train_image_filenames\n",
    "# missing_images = train_image_filenames - all_image_files\n",
    "\n",
    "# print(f\"Extra images in the folder: {len(extra_images)}\")\n",
    "# print(f\"Missing images: {len(missing_images)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fb7dbc2-0271-49be-84bc-c5ddae2f96b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Filter out rows where the image file doesn't exist\n",
    "# valid_image_paths = set(train_data_cleaned[\"Image_path\"].apply(lambda x: os.path.basename(x)))\n",
    "# all_images_in_folder = set(os.listdir(image_folder))\n",
    "\n",
    "# # Keep only records with matching image files\n",
    "# train_data_cleaned_filtered = train_data_cleaned[train_data_cleaned[\"Image_path\"].apply(\n",
    "#     lambda x: os.path.basename(x) in all_images_in_folder\n",
    "# )]\n",
    "\n",
    "# print(f\"Filtered train data shape: {train_data_cleaned_filtered.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4a7511b-5a47-4053-9ce8-a99610122724",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the number of unique image paths in the cleaned dataset and processed data\n",
    "print(f\"Unique image paths in train_data_cleaned: {train_data_cleaned['Image_path'].nunique()}\")\n",
    "print(f\"Unique image paths in processed train_combined: {train_combined['Image_path'].nunique()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc322c53-a995-4187-ad15-4747866492a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(test_combined['Image_path'].isnull().sum())  # To check if there are any missing paths\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00072dbc-029d-4d29-982d-5296777dd150",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for duplicates based on all columns in the combined dataset\n",
    "duplicates = train_combined[train_combined.duplicated(subset='Image_path', keep=False)]  # Keep=False to mark all duplicates\n",
    "print(duplicates)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cee80fd0-50e9-434b-81d9-f072ba9191fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the indexes of duplicated rows\n",
    "duplicate_indexes = train_combined[train_combined.duplicated(subset='Image_path',keep=False)].index\n",
    "print(\"Duplicate indexes:\", duplicate_indexes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17d94ead-a938-4958-8fb1-c10c521ef373",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Length of duplicate_indexes: {len(duplicate_indexes)}\")\n",
    "print(f\"Length of train_image_paths: {len(train_image_paths)}\")\n",
    "print(f\"Shape of train_combined: {train_combined.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "618c6689-4a47-476b-83db-cbfc2178c7c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_combined.to_csv('Fast_Furious_Insured/processed_data/train_combined.csv', index=False)\n",
    "test_combined.to_csv('Fast_Furious_Insured/processed_data/test_combined.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c8f469e-9338-4865-be31-785cb7450b5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_combined.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6c5b78b-b027-4f62-8698-d27b9bbf9ae6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save train_combined as pickle file\n",
    "train_combined.to_pickle('Fast_Furious_Insured/processed_data/final_train_data.pkl')\n",
    "\n",
    "# Save test_combined as pickle file\n",
    "test_combined.to_pickle('Fast_Furious_Insured/processed_data/final_test_data.pkl')\n",
    "\n",
    "print(\"Pickle files saved successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1789f912-8991-4612-a081-79a30e99e3a3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.11",
   "language": "python",
   "name": "python3.11"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
