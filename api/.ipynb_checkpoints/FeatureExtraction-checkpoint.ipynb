{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "541eefac-b772-4b8f-8c9e-33f11b7085b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tensorflow.keras.applications import ResNet50\n",
    "from tensorflow.keras.preprocessing.image import img_to_array\n",
    "from tensorflow.keras.applications.resnet50 import preprocess_input\n",
    "import os\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "import joblib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "123c6fc6-20b1-4b8a-803d-3a848811cb8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the cleaned structured data\n",
    "train_data_cleaned = pd.read_pickle('Fast_Furious_Insured/processed_data/train_data_cleaned.pkl')\n",
    "test_data_cleaned = pd.read_pickle('Fast_Furious_Insured/processed_data/test_data_cleaned.pkl')\n",
    "\n",
    "# Display the first few rows to confirm\n",
    "print(train_data_cleaned.head())\n",
    "print(test_data_cleaned.head())\n",
    "\n",
    "print(train_data_cleaned.shape)\n",
    "print(test_data_cleaned.shape)\n",
    "\n",
    "print(train_data_cleaned.dtypes)\n",
    "print(test_data_cleaned.dtypes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db4f63dd-c887-4eb6-a68c-b93df40b634a",
   "metadata": {},
   "source": [
    "### Turning Expiry_Date from an object to numeric value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "052ae644-8758-481f-941c-76783f31d7c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert 'Expiry_date' to datetime\n",
    "train_data_cleaned['Expiry_date'] = pd.to_datetime(train_data_cleaned['Expiry_date'], errors='coerce')\n",
    "test_data_cleaned['Expiry_date'] = pd.to_datetime(test_data_cleaned['Expiry_date'], errors='coerce')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35d37c65-49a3-4c4c-b0dd-7b97050f0aab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now, convert to numeric (e.g., number of days since a reference date, such as '2000-01-01')\n",
    "reference_date = pd.to_datetime('2000-01-01')\n",
    "\n",
    "train_data_cleaned['Expiry_date'] = (train_data_cleaned['Expiry_date'] - reference_date).dt.days\n",
    "test_data_cleaned['Expiry_date'] = (test_data_cleaned['Expiry_date'] - reference_date).dt.days"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "307aaf9f-04d2-4d4d-bb73-129f48e012c1",
   "metadata": {},
   "source": [
    "### Feature Scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d861e2d-e6e2-4221-8adc-3f624c93085b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define feature columns (excluding 'Amount' initially)\n",
    "feature_columns = ['Cost_of_vehicle', 'Min_coverage', 'Max_coverage', 'Expiry_date']\n",
    "\n",
    "# Initialize the scaler for features\n",
    "feature_scaler = StandardScaler()\n",
    "\n",
    "# Fit and transform feature columns\n",
    "train_data_cleaned[feature_columns] = feature_scaler.fit_transform(train_data_cleaned[feature_columns])\n",
    "\n",
    "# Scale 'Amount' separately using another scaler\n",
    "amount_scaler = StandardScaler()\n",
    "train_data_cleaned['Amount'] = amount_scaler.fit_transform(train_data_cleaned[['Amount']])\n",
    "\n",
    "# Save the scalers for later use\n",
    "joblib.dump(feature_scaler, 'feature_scaler.pkl')\n",
    "joblib.dump(amount_scaler, 'amount_scaler.pkl')\n",
    "\n",
    "print(train_data_cleaned)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a0edef1-3bda-4c40-80d7-e92403752f1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply the same transformation to test data (except 'Amount', which is not available)\n",
    "test_data_cleaned[feature_columns] = feature_scaler.transform(test_data_cleaned[feature_columns])\n",
    "print(test_data_cleaned)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50743c28-7544-418d-bebc-65c5eafc6f32",
   "metadata": {},
   "source": [
    "### One-Hot Encoding Insurance company"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b49cd85b-7ce9-4cc0-a103-595c520bc58f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the encoder\n",
    "encoder = OneHotEncoder(sparse=False, handle_unknown='ignore')\n",
    "\n",
    "# Fit the encoder on the training data and transform the 'Insurance_company' column in the train dataset\n",
    "train_data_encoded = encoder.fit_transform(train_data_cleaned[['Insurance_company']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14dfdb5e-9063-4b5e-a5cb-3801fa87f5c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the encoded data back to a DataFrame\n",
    "train_data_encoded = pd.DataFrame(train_data_encoded, columns=encoder.get_feature_names_out(['Insurance_company']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4280c896-effd-4f41-b7d3-c8055c1423e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concatenate the encoded features with the rest of the train data (drop the original 'Insurance_company' column)\n",
    "train_data_cleaned = pd.concat([train_data_cleaned.drop(columns=['Insurance_company']), train_data_encoded], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8b1a989-f5b1-40eb-86e6-98945774b378",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform the 'Insurance_company' column in the test dataset using the same encoder\n",
    "test_data_encoded = encoder.transform(test_data_cleaned[['Insurance_company']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab6ab188-81a6-42fa-bec6-eedba2e827a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the transformed data back to a DataFrame\n",
    "test_data_encoded = pd.DataFrame(test_data_encoded, columns=encoder.get_feature_names_out(['Insurance_company']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05805338-7011-4c45-bb61-382b5a30f047",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concatenate the encoded features with the rest of the test data (drop the original 'Insurance_company' column)\n",
    "test_data_cleaned = pd.concat([test_data_cleaned.drop(columns=['Insurance_company']), test_data_encoded], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31e378db-27c7-449e-9c6e-7711ccb8897a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train_data_cleaned.shape)\n",
    "print(test_data_cleaned.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1397b33-fcd2-4443-9436-ae9d1715c9a3",
   "metadata": {},
   "source": [
    "### Image Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e63de076-08a9-490c-b8e3-dd56704f1418",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "\n",
    "# # Folder containing all images\n",
    "# image_folder = \"Fast_Furious_Insured/images/train_images\"\n",
    "\n",
    "# # Get the list of images that are still relevant\n",
    "# valid_images = set(train_data_cleaned[\"Image_path\"].apply(os.path.basename))  # Extract filenames only\n",
    "\n",
    "# # List all files in the folder\n",
    "# all_images = set(os.listdir(image_folder))\n",
    "\n",
    "# # Find extra images that need to be deleted\n",
    "# extra_images = all_images - valid_images\n",
    "\n",
    "# # Delete extra images\n",
    "# for img in extra_images:\n",
    "#     img_path = os.path.join(image_folder, img)\n",
    "#     os.remove(img_path)  # Deletes the file\n",
    "#     print(f\"Deleted: {img_path}\")\n",
    "\n",
    "# print(f\"âœ… Removed {len(extra_images)} unnecessary images.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7038ddcd-e352-4be7-b21d-10a5d937e4a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#No. of Images\n",
    "train_images_path = \"Fast_Furious_Insured/images/train_images\"\n",
    "test_images_path = \"Fast_Furious_Insured/images/test_images\"\n",
    "\n",
    "num_train_images = len(os.listdir(train_images_path))\n",
    "num_test_images = len(os.listdir(test_images_path))\n",
    "\n",
    "print(f\"Number of training images: {num_train_images}\")\n",
    "print(f\"Number of test images: {num_test_images}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "052269cf-3830-4f84-bb77-268c31064730",
   "metadata": {},
   "outputs": [],
   "source": [
    "IMG_SIZE = (224, 224)  # ResNet50 requires images to be 224x224\n",
    "\n",
    "def preprocess_image(image_path):\n",
    "    # Load the image using OpenCV\n",
    "    image = cv2.imread(image_path)\n",
    "    \n",
    "    # Resize the image to the target size (224x224 for ResNet50)\n",
    "    image_resized = cv2.resize(image, IMG_SIZE)\n",
    "    \n",
    "    # Convert image to float32 for normalization\n",
    "    image_normalized = image_resized.astype('float32') / 255.0\n",
    "    \n",
    "    # Preprocess the image using ResNet50 preprocessing (mean subtraction, etc.)\n",
    "    image_processed = preprocess_input(image_normalized)\n",
    "    \n",
    "    return image_processed\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6642ae32-b0a2-4e81-817d-4a4fd0f4e57c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the pre-trained ResNet50 model (exclude top layers for feature extraction)\n",
    "base_model = ResNet50(weights='imagenet', include_top=False, pooling='avg')\n",
    "\n",
    "def extract_image_features(image_path):\n",
    "    # Preprocess the image\n",
    "    image_processed = preprocess_image(image_path)\n",
    "    \n",
    "    # Expand dimensions for batch processing (ResNet50 expects a batch)\n",
    "    image_batch = np.expand_dims(image_processed, axis=0)\n",
    "    \n",
    "    # Extract features using the ResNet50 model\n",
    "    features = base_model.predict(image_batch)\n",
    "    \n",
    "    # Flatten the feature vector\n",
    "    return features.flatten()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe38d7d3-e36e-449f-8642-f170d0f5199a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def combine_features(image_paths, structured_data):\n",
    "    image_features = []\n",
    "    \n",
    "    # Iterate over all image paths and extract features\n",
    "    for path in image_paths:\n",
    "        features = extract_image_features(path)\n",
    "        image_features.append(features)\n",
    "    \n",
    "    # Convert image features into a DataFrame\n",
    "    image_features_df = pd.DataFrame(image_features, index=structured_data.index)\n",
    "\n",
    "     # Print to check before merging\n",
    "    print(\"Image Features Shape:\", image_features_df.shape)\n",
    "    print(\"Structured Data Shape:\", structured_data.shape)\n",
    "    \n",
    "    # Concatenate image features with structured data (merge on index)\n",
    "    combined_data = pd.concat([structured_data.reset_index(drop=True), image_features_df], axis=1)\n",
    "    \n",
    "    return combined_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f60f2605-541b-43ff-9c99-e0892239009c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the image paths from the 'Image_Path' column\n",
    "train_image_paths = train_data_cleaned['Image_path'].tolist()\n",
    "test_image_paths = test_data_cleaned['Image_path'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87fa90b4-d362-446c-abd9-8b962db6dd3c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Combine image features with the structured data\n",
    "train_combined = combine_features(train_image_paths, train_data_cleaned)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f04eae5-02dd-4bcb-9e56-ccc5191ed502",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "test_combined = combine_features(test_image_paths, test_data_cleaned)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14ddeacd-73ad-4774-8281-5b81a8acd7b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_combined.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f97586a4-421c-4be5-9e4a-1054b0405d53",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_combined.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8de7fbc1-1a50-4908-a29b-aa3a344f56a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Extract image filenames from DataFrame\n",
    "# train_images_used = set(train_data_cleaned[\"Image_path\"].apply(lambda x: x.split(\"/\")[-1]))\n",
    "\n",
    "# # List actual image files in the folder\n",
    "# import os\n",
    "# image_folder = \"Fast_Furious_Insured/images/train_images\"\n",
    "# all_image_files = set(os.listdir(image_folder))\n",
    "\n",
    "# # Find extra images that are not in train_data_cleaned\n",
    "# extra_images = all_image_files - train_images_used\n",
    "\n",
    "# print(f\"Extra images processed: {len(extra_images)}\")\n",
    "# print(f\"Sample extra images: {list(extra_images)[:5]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "679a71eb-c21e-4a4d-8c6e-274ec922ffa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(train_image_paths), len(set(train_image_paths)))  # Both should be 1310"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff54491d-44cd-4982-87e6-93480e793933",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Get the list of image filenames in the folder\n",
    "image_folder = \"Fast_Furious_Insured/images/train_images\"\n",
    "all_image_files = set(os.listdir(image_folder))\n",
    "\n",
    "# Extract filenames from the train dataset\n",
    "train_image_filenames = set(train_data_cleaned[\"Image_path\"].apply(lambda x: os.path.basename(x)))\n",
    "\n",
    "# Identify extra images (those in the folder but not in the dataset)\n",
    "extra_images = all_image_files - train_image_filenames\n",
    "missing_images = train_image_filenames - all_image_files\n",
    "\n",
    "print(f\"Extra images in the folder: {len(extra_images)}\")\n",
    "print(f\"Missing images: {len(missing_images)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fb7dbc2-0271-49be-84bc-c5ddae2f96b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter out rows where the image file doesn't exist\n",
    "valid_image_paths = set(train_data_cleaned[\"Image_path\"].apply(lambda x: os.path.basename(x)))\n",
    "all_images_in_folder = set(os.listdir(image_folder))\n",
    "\n",
    "# Keep only records with matching image files\n",
    "train_data_cleaned_filtered = train_data_cleaned[train_data_cleaned[\"Image_path\"].apply(\n",
    "    lambda x: os.path.basename(x) in all_images_in_folder\n",
    ")]\n",
    "\n",
    "print(f\"Filtered train data shape: {train_data_cleaned_filtered.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4a7511b-5a47-4053-9ce8-a99610122724",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the number of unique image paths in the cleaned dataset and processed data\n",
    "print(f\"Unique image paths in train_data_cleaned: {train_data_cleaned['Image_path'].nunique()}\")\n",
    "print(f\"Unique image paths in processed train_combined: {train_combined['Image_path'].nunique()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc322c53-a995-4187-ad15-4747866492a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(test_combined['Image_path'].isnull().sum())  # To check if there are any missing paths\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00072dbc-029d-4d29-982d-5296777dd150",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for duplicates based on all columns in the combined dataset\n",
    "duplicates = train_combined[train_combined.duplicated(subset='Image_path', keep=False)]  # Keep=False to mark all duplicates\n",
    "print(duplicates)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cee80fd0-50e9-434b-81d9-f072ba9191fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the indexes of duplicated rows\n",
    "duplicate_indexes = train_combined[train_combined.duplicated(subset='Image_path',keep=False)].index\n",
    "print(\"Duplicate indexes:\", duplicate_indexes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17d94ead-a938-4958-8fb1-c10c521ef373",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Length of duplicate_indexes: {len(duplicate_indexes)}\")\n",
    "print(f\"Length of train_image_paths: {len(train_image_paths)}\")\n",
    "print(f\"Shape of train_combined: {train_combined.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65fa3aa2-40ed-412d-8a2e-467b09fd6bbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop duplicates from train_combined based on the image paths (or any other criteria)\n",
    "train_combined = train_combined.drop_duplicates(subset='Image_path',keep=False)\n",
    "\n",
    "# Verify the new shape\n",
    "print(f\"New shape of train_combined: {train_combined.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "618c6689-4a47-476b-83db-cbfc2178c7c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_combined.to_csv('train_combined.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c8f469e-9338-4865-be31-785cb7450b5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_combined.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6c5b78b-b027-4f62-8698-d27b9bbf9ae6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save train_combined as pickle file\n",
    "train_combined.to_pickle('Fast_Furious_Insured/processed_data/final_train_data.pkl')\n",
    "\n",
    "# Save test_combined as pickle file\n",
    "test_combined.to_pickle('Fast_Furious_Insured/processed_data/final_test_data.pkl')\n",
    "\n",
    "print(\"Pickle files saved successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67d21904-adfe-4ab2-99e0-ae72d723d9c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_combined.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1789f912-8991-4612-a081-79a30e99e3a3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.11",
   "language": "python",
   "name": "python3.11"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
